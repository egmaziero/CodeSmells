{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Arch #1\n",
    "## Embedding->CNN->LSTM->Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/erickmaziero/virtualenvs/CodeSmells_env/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data load and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trainset</th>\n",
       "      <th>polarity</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>4</td>\n",
       "      <td>It must be remembered that the Gammera movies,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>No Holds Barred is a movie that should in no w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>10</td>\n",
       "      <td>I consider myself a huge movie buff. I was sic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>9</td>\n",
       "      <td>I caught this one on cable and I was very surp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>Turkish Cinema has a big problem. Directors ar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  trainset polarity                                             review\n",
       "0     test        4  It must be remembered that the Gammera movies,...\n",
       "1     test        1  No Holds Barred is a movie that should in no w...\n",
       "2    train       10  I consider myself a huge movie buff. I was sic...\n",
       "3     test        9  I caught this one on cable and I was very surp...\n",
       "4    train        1  Turkish Cinema has a big problem. Directors ar..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = '../data/aclImdb'\n",
    "train_positive_files = ['train/pos/'+f for f in os.listdir(dataset_path+'/train/pos') \\\n",
    "                        if os.path.isfile(os.path.join(dataset_path+'/train/pos', f))]\n",
    "train_negative_files = ['train/neg/'+f for f in os.listdir(dataset_path+'/train/neg') \\\n",
    "                        if os.path.isfile(os.path.join(dataset_path+'/train/neg', f))]\n",
    "test_positive_files = ['test/pos/'+f for f in os.listdir(dataset_path+'/test/pos') \\\n",
    "                       if os.path.isfile(os.path.join(dataset_path+'/test/pos', f))]\n",
    "test_negative_files = ['test/neg/'+f for f in os.listdir(dataset_path+'/test/neg') \\\n",
    "                       if os.path.isfile(os.path.join(dataset_path+'/test/neg', f))]\n",
    "all_files = list(set().union(train_positive_files,train_negative_files, test_positive_files, test_negative_files))\n",
    "\n",
    "dataset = {'trainset':[], \n",
    "           'polarity':[], \n",
    "           'review':[]}\n",
    "\n",
    "for file in all_files:\n",
    "    polarity = file.split('.')[0].split('_')[1]\n",
    "    with open(os.path.join(dataset_path, file), 'r') as text_file:\n",
    "        dataset['trainset'].append(file.split('/')[0])\n",
    "        dataset['polarity'].append(polarity)\n",
    "        dataset['review'].append(text_file.readlines()[0])\n",
    "dataframe = pd.DataFrame(data=dataset)\n",
    "\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 's', 't', 'can', 'will']\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords = [word for word in stopwords if word not in ['very', 'no', 'nor', 'not', 'few', 'more',\n",
    "                                                        'most', 'just', 'doesn',  'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]]\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trainset</th>\n",
       "      <th>polarity</th>\n",
       "      <th>review</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>4</td>\n",
       "      <td>It must be remembered that the Gammera movies,...</td>\n",
       "      <td>must rememb gammera movi , like mani first-ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>No Holds Barred is a movie that should in no w...</td>\n",
       "      <td>no hold bar movi should no way ever taken seri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>10</td>\n",
       "      <td>I consider myself a huge movie buff. I was sic...</td>\n",
       "      <td>consid huge movi buff . sick couch pop film . ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>9</td>\n",
       "      <td>I caught this one on cable and I was very surp...</td>\n",
       "      <td>caught one cabl veri surpris . steadi direct g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>Turkish Cinema has a big problem. Directors ar...</td>\n",
       "      <td>turkish cinema big problem . director n't inte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  trainset polarity                                             review  \\\n",
       "0     test        4  It must be remembered that the Gammera movies,...   \n",
       "1     test        1  No Holds Barred is a movie that should in no w...   \n",
       "2    train       10  I consider myself a huge movie buff. I was sic...   \n",
       "3     test        9  I caught this one on cable and I was very surp...   \n",
       "4    train        1  Turkish Cinema has a big problem. Directors ar...   \n",
       "\n",
       "                                        clean_review  \n",
       "0  must rememb gammera movi , like mani first-ser...  \n",
       "1  no hold bar movi should no way ever taken seri...  \n",
       "2  consid huge movi buff . sick couch pop film . ...  \n",
       "3  caught one cabl veri surpris . steadi direct g...  \n",
       "4  turkish cinema big problem . director n't inte...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_reviews(review):\n",
    "    tokens = nltk.tokenize.word_tokenize(review.lower())\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens if token not in stopwords]\n",
    "    return \" \".join(stemmed_tokens)\n",
    "\n",
    "dataframe['clean_review'] = dataframe['review'].apply(preprocess_reviews)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_pickle('../data/dataframe_processed_reviews.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews = dataframe[dataframe['trainset'] == 'train']['clean_review'].values.tolist()\n",
    "train_classes = dataframe[dataframe['trainset'] == 'train']['polarity'].values.tolist()\n",
    "labels_train = np.array(train_classes)\n",
    "test_reviews = dataframe[dataframe['trainset'] == 'test']['clean_review'].values.tolist()\n",
    "test_classes = dataframe[dataframe['trainset'] == 'test']['polarity'].values.tolist()\n",
    "labels_test = np.array(test_classes)\n",
    "\n",
    "labels_categorical_train = to_categorical(labels_train, num_classes=11)\n",
    "labels_categorical_test = to_categorical(labels_test, num_classes=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89401\n",
      "1796\n"
     ]
    }
   ],
   "source": [
    "# find the vocab size\n",
    "vocab = {}\n",
    "max_length = 0\n",
    "for review in train_reviews:\n",
    "    tokens = review.split()\n",
    "    \n",
    "    if len(tokens) > max_length:\n",
    "        max_length = len(tokens)\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            vocab[token] += 1\n",
    "        else:\n",
    "            vocab[token] = 0\n",
    "\n",
    "print(len(vocab))\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[75882 44527 52217 ...     0     0     0]\n",
      " [28224 70033 50182 ...     0     0     0]\n",
      " [82201 81466 44722 ...     0     0     0]\n",
      " ...\n",
      " [54567  4434 12497 ...     0     0     0]\n",
      " [ 5162 59188 11044 ...     0     0     0]\n",
      " [50936 67188 85032 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab) + 100\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in train_reviews]\n",
    "print(encoded_docs)\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[88110 39503 66478 ...     0     0     0]\n",
      " [60865 51889 30522 ...     0     0     0]\n",
      " [20294  6003 82966 ...     0     0     0]\n",
      " ...\n",
      " [71454 52925 22192 ...     0     0     0]\n",
      " [ 6451  6003 58122 ...     0     0     0]\n",
      " [84333 31870 83348 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab) + 100\n",
    "encoded_docs_test = [one_hot(d, vocab_size) for d in test_reviews]\n",
    "print(encoded_docs_test)\n",
    "padded_docs_test = pad_sequences(encoded_docs_test, maxlen=max_length, padding='post')\n",
    "print(padded_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################# Model 1 ###############\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1796, 128)         11456128  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 229888)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                7356448   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 11)                363       \n",
      "=================================================================\n",
      "Total params: 18,812,939\n",
      "Trainable params: 18,812,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Embedding(vocab_size, 128, input_length=max_length))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(32, activation='relu'))\n",
    "model1.add(Dense(11, activation='softmax'))\n",
    "# compile the model\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# summarize the model\n",
    "print('################# Model 1 ###############')\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 188s 9ms/step - loss: 1.9732 - acc: 0.2994 - val_loss: 1.7744 - val_acc: 0.3580\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 190s 9ms/step - loss: 1.4924 - acc: 0.4122 - val_loss: 1.7258 - val_acc: 0.3442\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 190s 10ms/step - loss: 1.1498 - acc: 0.5129 - val_loss: 1.9362 - val_acc: 0.3254\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 205s 10ms/step - loss: 0.9230 - acc: 0.6166 - val_loss: 2.3743 - val_acc: 0.2948\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 180s 9ms/step - loss: 0.7694 - acc: 0.6959 - val_loss: 2.9505 - val_acc: 0.2866\n",
      "Accuracy: 27.252000\n"
     ]
    }
   ],
   "source": [
    "model1.fit(padded_docs, labels_categorical_train, epochs=5, verbose=1, validation_split=0.2)\n",
    "# evaluate the model\n",
    "loss, accuracy = model1.evaluate(padded_docs_test, labels_categorical_test, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################# Model 2 ###############\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 1796, 128)         11456128  \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1792, 64)          41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 448, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 11)                1111      \n",
      "=================================================================\n",
      "Total params: 11,564,263\n",
      "Trainable params: 11,564,263\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(vocab_size, 128, input_length=max_length))\n",
    "model2.add(Conv1D(64, 5, activation='relu'))\n",
    "model2.add(MaxPooling1D(pool_size=4))\n",
    "model2.add(LSTM(100))\n",
    "model2.add(Dense(11, activation='softmax'))\n",
    "# compile the model\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print('################# Model 2 ###############')\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 687s 34ms/step - loss: 2.0451 - acc: 0.1953 - val_loss: 2.0315 - val_acc: 0.2046\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 715s 36ms/step - loss: 2.0333 - acc: 0.1993 - val_loss: 2.0330 - val_acc: 0.2046\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 678s 34ms/step - loss: 2.0323 - acc: 0.1986 - val_loss: 2.0338 - val_acc: 0.1884\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 642s 32ms/step - loss: 2.0320 - acc: 0.1974 - val_loss: 2.0296 - val_acc: 0.2046\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 678s 34ms/step - loss: 2.0307 - acc: 0.1983 - val_loss: 2.0303 - val_acc: 0.2046\n",
      "Accuracy: 20.088000\n"
     ]
    }
   ],
   "source": [
    "model2.fit(padded_docs, labels_categorical_train, epochs=5, verbose=1, validation_split=0.2)\n",
    "# evaluate the model\n",
    "loss, accuracy = model2.evaluate(padded_docs_test, labels_categorical_test, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
